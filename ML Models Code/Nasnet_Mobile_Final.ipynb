{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCujd9kyFvL8",
        "outputId": "2978df7a-6dec-4041-b8c5-2f31093490fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF version: 2.8.2\n",
            "Hub version: 0.12.0\n",
            "GPU is available\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "import os\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "print(\"TF version:\", tf.__version__)\n",
        "print(\"Hub version:\", hub.__version__)\n",
        "print(\"GPU is\", \"available\" if tf.config.list_physical_devices('GPU') else \"NOT AVAILABLE\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"nasnet_mobile\"\n",
        "model_handle = \"https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4\"\n",
        "pixels = 224\n",
        "\n",
        "print(f\"Selected model: {model_name} : {model_handle}\")\n",
        "\n",
        "IMAGE_SIZE = (pixels, pixels)\n",
        "print(f\"Input size {IMAGE_SIZE}\")\n",
        "\n",
        "BATCH_SIZE = 16"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f75vpOIfGGNO",
        "outputId": "8ef08025-f42d-4d89-e84a-e7541b448de4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected model: nasnet_mobile : https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4\n",
            "Input size (224, 224)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upNKaNz7G2ZX",
        "outputId": "4b23973c-2ae5-4c26-b5e4-525572773ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "filename = \"/content/drive/MyDrive/dataset.zip\"\n",
        "with zipfile.ZipFile(filename, 'r') as zipp:\n",
        "  zipp.extractall()\n",
        "  zipp.close()"
      ],
      "metadata": {
        "id": "E8U4jnX-HBQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = '/content/Augumented/train'"
      ],
      "metadata": {
        "id": "WB_n8s6hHHat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_dataset(subset):\n",
        "  return tf.keras.preprocessing.image_dataset_from_directory(\n",
        "      data_dir,\n",
        "      validation_split=.20,\n",
        "      subset=subset,\n",
        "      label_mode=\"categorical\",\n",
        "      # Seed needs to provided when using validation_split and shuffle = True.\n",
        "      # A fixed seed is used so that the validation set is stable across runs.\n",
        "      seed=123,\n",
        "      image_size=IMAGE_SIZE,\n",
        "      batch_size=1)\n",
        "\n",
        "train_ds = build_dataset(\"training\")\n",
        "class_names = tuple(train_ds.class_names)\n",
        "train_size = train_ds.cardinality().numpy()\n",
        "train_ds = train_ds.unbatch().batch(BATCH_SIZE)\n",
        "train_ds = train_ds.repeat()\n",
        "\n",
        "normalization_layer = tf.keras.layers.Rescaling(1. / 255)\n",
        "preprocessing_model = tf.keras.Sequential([normalization_layer])\n",
        "do_data_augmentation = True\n",
        "if do_data_augmentation:\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomRotation(40))\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomTranslation(0, 0.2))\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomTranslation(0.2, 0))\n",
        "  # Like the old tf.keras.preprocessing.image.ImageDataGenerator(),\n",
        "  # image sizes are fixed when reading, and then a random zoom is applied.\n",
        "  # If all training inputs are larger than image_size, one could also use\n",
        "  # RandomCrop with a batch size of 1 and rebatch later.\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomZoom(0.2, 0.2))\n",
        "  preprocessing_model.add(\n",
        "      tf.keras.layers.RandomFlip(mode=\"horizontal\"))\n",
        "train_ds = train_ds.map(lambda images, labels:\n",
        "                        (preprocessing_model(images), labels))\n",
        "\n",
        "val_ds = build_dataset(\"validation\")\n",
        "valid_size = val_ds.cardinality().numpy()\n",
        "val_ds = val_ds.unbatch().batch(BATCH_SIZE)\n",
        "val_ds = val_ds.map(lambda images, labels:\n",
        "                    (normalization_layer(images), labels))"
      ],
      "metadata": {
        "id": "yA01LbfKHHmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98b8c6d1-45db-4d26-ce55-80a3034ba2c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 149159 files belonging to 51 classes.\n",
            "Using 119328 files for training.\n",
            "Found 149159 files belonging to 51 classes.\n",
            "Using 29831 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "do_fine_tuning = True"
      ],
      "metadata": {
        "id": "yCGT6SHPHHpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Building model with\", model_handle)\n",
        "model = tf.keras.Sequential([\n",
        "    # Explicitly define the input shape so the model can be properly\n",
        "    # loaded by the TFLiteConverter\n",
        "    tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE + (3,)),\n",
        "    hub.KerasLayer(model_handle, trainable=do_fine_tuning),\n",
        "    tf.keras.layers.Dropout(rate=0.2),\n",
        "    tf.keras.layers.Dense(len(class_names),\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
        "])\n",
        "model.build((None,)+IMAGE_SIZE+(3,))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "TZOVfzoOHHtF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a545f44-e164-4845-c509-25f21b048ee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Building model with https://tfhub.dev/google/imagenet/nasnet_mobile/feature_vector/4\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " keras_layer (KerasLayer)    (None, 1056)              4269716   \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1056)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 51)                53907     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,323,623\n",
            "Trainable params: 4,286,885\n",
            "Non-trainable params: 36,738\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.SGD(learning_rate=0.005, momentum=0.9), \n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
        "  metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "8ynBFTjtHHwm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps_per_epoch = train_size // BATCH_SIZE\n",
        "validation_steps = valid_size // BATCH_SIZE\n",
        "hist = model.fit(\n",
        "    train_ds,\n",
        "    epochs=4, steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=val_ds,\n",
        "    validation_steps=validation_steps).history"
      ],
      "metadata": {
        "id": "VqJFBPbbHHz2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9627488e-74e8-4124-ba5e-7bc2380f4e26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/4\n",
            "7458/7458 [==============================] - 4040s 537ms/step - loss: 1.7131 - accuracy: 0.8009 - val_loss: 1.4705 - val_accuracy: 0.8732\n",
            "Epoch 2/4\n",
            "7458/7458 [==============================] - 3981s 534ms/step - loss: 1.4190 - accuracy: 0.8951 - val_loss: 1.4191 - val_accuracy: 0.8911\n",
            "Epoch 3/4\n",
            "7458/7458 [==============================] - 3978s 533ms/step - loss: 1.3463 - accuracy: 0.9179 - val_loss: 1.3403 - val_accuracy: 0.9154\n",
            "Epoch 4/4\n",
            "2601/7458 [=========>....................] - ETA: 41:46 - loss: 1.3041 - accuracy: 0.9315"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.ylabel(\"Loss (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,2])\n",
        "plt.plot(hist[\"loss\"])\n",
        "plt.plot(hist[\"val_loss\"])\n",
        "\n",
        "plt.figure()\n",
        "plt.ylabel(\"Accuracy (training and validation)\")\n",
        "plt.xlabel(\"Training Steps\")\n",
        "plt.ylim([0,1])\n",
        "plt.plot(hist[\"accuracy\"])\n",
        "plt.plot(hist[\"val_accuracy\"])"
      ],
      "metadata": {
        "id": "C01743iyHH3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x, y = next(iter(val_ds))\n",
        "image = x[0, :, :, :]\n",
        "true_index = np.argmax(y[0])\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Expand the validation image to (1, 224, 224, 3) before predicting the label\n",
        "prediction_scores = model.predict(np.expand_dims(image, axis=0))\n",
        "predicted_index = np.argmax(prediction_scores)\n",
        "print(\"True label: \" + class_names[true_index])\n",
        "print(\"Predicted label: \" + class_names[predicted_index])"
      ],
      "metadata": {
        "id": "9THih7AiHH6V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from PIL import Image\n",
        "# import numpy as np\n",
        "\n",
        "# im1 = Image.open('/content/Potato.png')\n",
        "# im1 = im1.resize((pixels,pixels))\n",
        "# im1 = np.reshape(im1,newshape=(1,pixels,pixels,3))\n",
        "# x_test = np.array(im1)\n",
        "# x_test = x_test/255\n",
        "# x_test.dtype = np.float32"
      ],
      "metadata": {
        "id": "xnLWY5E5HH-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "# # load image via tf.io\n",
        "# img = tf.io.read_file(\"/content/Bannana (2).jpg\")\n",
        "\n",
        "# # convert to tensor (specify 3 channels explicitly since png files contains additional alpha channel)\n",
        "# # set the dtypes to align with pytorch for comparison since it will use uint8 by default\n",
        "# tensor = tf.io.decode_image(img, channels=3, dtype=tf.dtypes.float32)\n",
        "# # (384, 470, 3)\n",
        "\n",
        "# # resize tensor to 224 x 224\n",
        "# tensor = tf.image.resize(tensor, [224, 224])\n",
        "# # (224, 224, 3)\n",
        "\n",
        "# # add another dimension at the front to get NHWC shape\n",
        "# input_tensor = tf.expand_dims(tensor, axis=0)\n",
        "# # (1, 224, 224, 3)"
      ],
      "metadata": {
        "id": "MxMdncnxTCzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class_names[np.argmax(model.predict(input_tensor))]"
      ],
      "metadata": {
        "id": "v6bd1HsMHIBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model.predict(input_tensor)[0][40]"
      ],
      "metadata": {
        "id": "U4TraW0nNYzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class_names[9]"
      ],
      "metadata": {
        "id": "O16hVUyRNbOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names"
      ],
      "metadata": {
        "id": "97RtY5pyg5NP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.save(\"/content/drive/MyDrive/mobilenet_v3.h5\")"
      ],
      "metadata": {
        "id": "FI4x7pNeO0PC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = tf.keras.models.load_model('/content/drive/MyDrive/mobilenet_v3')"
      ],
      "metadata": {
        "id": "9EKK0b4hQNrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "saved_model_path = \"/content/drive/MyDrive/Models/Nasnet_Augumented\"\n",
        "tf.saved_model.save(model, saved_model_path)"
      ],
      "metadata": {
        "id": "cHa3N1xJHIEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimize_lite_model = False \n",
        "num_calibration_examples = 0  \n",
        "representative_dataset = None\n",
        "if optimize_lite_model and num_calibration_examples:\n",
        "  # Use a bounded number of training examples without labels for calibration.\n",
        "  # TFLiteConverter expects a list of input tensors, each with batch size 1.\n",
        "  representative_dataset = lambda: itertools.islice(\n",
        "      ([image[None, ...]] for batch, _ in train_ds for image in batch),\n",
        "      num_calibration_examples)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
        "if optimize_lite_model:\n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  if representative_dataset:  # This is optional, see above.\n",
        "    converter.representative_dataset = representative_dataset\n",
        "lite_model_content = converter.convert()\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Tflite_Models/Nasnet_Augumented.tflite\", \"wb\") as f:\n",
        "  f.write(lite_model_content)\n",
        "print(\"Wrote %sTFLite model of %d bytes.\" %\n",
        "      (\"optimized \" if optimize_lite_model else \"\", len(lite_model_content)))"
      ],
      "metadata": {
        "id": "aN1cEX-eHIHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=lite_model_content)\n",
        "# This little helper wraps the TFLite Interpreter as a numpy-to-numpy function.\n",
        "def lite_model(images):\n",
        "  interpreter.allocate_tensors()\n",
        "  interpreter.set_tensor(interpreter.get_input_details()[0]['index'], images)\n",
        "  interpreter.invoke()\n",
        "  return interpreter.get_tensor(interpreter.get_output_details()[0]['index'])"
      ],
      "metadata": {
        "id": "_idYLduSQ2cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  \n",
        "num_eval_examples = 0  \n",
        "eval_dataset = ((image, label)  # TFLite expects batch size 1.\n",
        "                for batch in train_ds\n",
        "                for (image, label) in zip(*batch))\n",
        "count = 0\n",
        "count_lite_tf_agree = 0\n",
        "count_lite_correct = 0\n",
        "for image, label in eval_dataset:\n",
        "  probs_lite = lite_model(image[None, ...])[0]\n",
        "  probs_tf = model(image[None, ...]).numpy()[0]\n",
        "  y_lite = np.argmax(probs_lite)\n",
        "  y_tf = np.argmax(probs_tf)\n",
        "  y_true = np.argmax(label)\n",
        "  count +=1\n",
        "  if y_lite == y_tf: count_lite_tf_agree += 1\n",
        "  if y_lite == y_true: count_lite_correct += 1\n",
        "  if count >= num_eval_examples: break\n",
        "print(\"TFLite model agrees with original model on %d of %d examples (%g%%).\" %\n",
        "      (count_lite_tf_agree, count, 100.0 * count_lite_tf_agree / count))\n",
        "print(\"TFLite model is accurate on %d of %d examples (%g%%).\" %\n",
        "      (count_lite_correct, count, 100.0 * count_lite_correct / count))"
      ],
      "metadata": {
        "id": "x7B8bA1qHILK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names[np.argmax(model(input_tensor).numpy()[0])]"
      ],
      "metadata": {
        "id": "S9880vS4HIOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names[np.argmax(lite_model(input_tensor)[0])]"
      ],
      "metadata": {
        "id": "pFa-dFYWUy1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# load image via tf.io\n",
        "img = tf.io.read_file(\"/content/drive/MyDrive/RealImages/RedApple.jpg\")\n",
        "\n",
        "# convert to tensor (specify 3 channels explicitly since png files contains additional alpha channel)\n",
        "# set the dtypes to align with pytorch for comparison since it will use uint8 by default\n",
        "tensor = tf.io.decode_image(img, channels=3, dtype=tf.dtypes.float32)\n",
        "# (384, 470, 3)\n",
        "\n",
        "# resize tensor to 224 x 224\n",
        "tensor = tf.image.resize(tensor, [224, 224])\n",
        "# (224, 224, 3)\n",
        "\n",
        "# add another dimension at the front to get NHWC shape\n",
        "input_tensor = tf.expand_dims(tensor, axis=0)\n",
        "# (1, 224, 224, 3)"
      ],
      "metadata": {
        "id": "Crhfyyqku0j7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_path=\"/content/drive/MyDrive/Tflite_Models/Nasnet_Augumented.tflite\")\n",
        "# This little helper wraps the TFLite Interpreter as a numpy-to-numpy function.\n",
        "def lite_model_1(images):\n",
        "  interpreter.allocate_tensors()\n",
        "  interpreter.set_tensor(interpreter.get_input_details()[0]['index'], images)\n",
        "  interpreter.invoke()\n",
        "  return interpreter.get_tensor(interpreter.get_output_details()[0]['index'])"
      ],
      "metadata": {
        "id": "UDRGtvYhHIUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_names[np.argmax(lite_model_1(input_tensor)[0])]"
      ],
      "metadata": {
        "id": "h9YOmISCHIRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (lite_model_1(input_tensor)[0])"
      ],
      "metadata": {
        "id": "kJHcXjeMHIXK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}